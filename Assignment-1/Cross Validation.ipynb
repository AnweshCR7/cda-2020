{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4035 - Cyber Data Analytics\n",
    "## Lab 1 - Fraud data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Number : 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student 1 \n",
    "### Name : Aditya Kunar\n",
    "### ID : 5074274"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student 2\n",
    "### Name : Anwesh Marwade\n",
    "### ID : 5052068"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import re\n",
    "from random import randint\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_for_student_case.csv\" #modify the path here for wherever the data is.\n",
    "df1 = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txid</th>\n",
       "      <th>bookingdate</th>\n",
       "      <th>issuercountrycode</th>\n",
       "      <th>txvariantcode</th>\n",
       "      <th>bin</th>\n",
       "      <th>amount</th>\n",
       "      <th>currencycode</th>\n",
       "      <th>shoppercountrycode</th>\n",
       "      <th>shopperinteraction</th>\n",
       "      <th>simple_journal</th>\n",
       "      <th>cardverificationcodesupplied</th>\n",
       "      <th>cvcresponsecode</th>\n",
       "      <th>creationdate</th>\n",
       "      <th>accountcode</th>\n",
       "      <th>mail_id</th>\n",
       "      <th>ip_id</th>\n",
       "      <th>card_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-11-09 14:26:51</td>\n",
       "      <td>MX</td>\n",
       "      <td>mccredit</td>\n",
       "      <td>530056.0</td>\n",
       "      <td>64800.0</td>\n",
       "      <td>MXN</td>\n",
       "      <td>MX</td>\n",
       "      <td>Ecommerce</td>\n",
       "      <td>Chargeback</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-01 23:03:11</td>\n",
       "      <td>MexicoAccount</td>\n",
       "      <td>email68370</td>\n",
       "      <td>ip111778</td>\n",
       "      <td>card184798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-11-09 14:27:38</td>\n",
       "      <td>MX</td>\n",
       "      <td>mccredit</td>\n",
       "      <td>547046.0</td>\n",
       "      <td>44900.0</td>\n",
       "      <td>MXN</td>\n",
       "      <td>MX</td>\n",
       "      <td>Ecommerce</td>\n",
       "      <td>Chargeback</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-02 04:50:55</td>\n",
       "      <td>MexicoAccount</td>\n",
       "      <td>email101299</td>\n",
       "      <td>ip78749</td>\n",
       "      <td>card151595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-11-23 16:34:16</td>\n",
       "      <td>MX</td>\n",
       "      <td>mccredit</td>\n",
       "      <td>528843.0</td>\n",
       "      <td>149900.0</td>\n",
       "      <td>MXN</td>\n",
       "      <td>MX</td>\n",
       "      <td>Ecommerce</td>\n",
       "      <td>Chargeback</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-02 14:30:28</td>\n",
       "      <td>MexicoAccount</td>\n",
       "      <td>email278604</td>\n",
       "      <td>ip70594</td>\n",
       "      <td>card242142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-11-23 16:34:51</td>\n",
       "      <td>MX</td>\n",
       "      <td>mccredit</td>\n",
       "      <td>547146.0</td>\n",
       "      <td>109900.0</td>\n",
       "      <td>MXN</td>\n",
       "      <td>MX</td>\n",
       "      <td>Ecommerce</td>\n",
       "      <td>Chargeback</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-03 07:53:37</td>\n",
       "      <td>MexicoAccount</td>\n",
       "      <td>email47409</td>\n",
       "      <td>ip113648</td>\n",
       "      <td>card181744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-11-09 14:26:08</td>\n",
       "      <td>MX</td>\n",
       "      <td>visaclassic</td>\n",
       "      <td>477291.0</td>\n",
       "      <td>89900.0</td>\n",
       "      <td>MXN</td>\n",
       "      <td>MX</td>\n",
       "      <td>Ecommerce</td>\n",
       "      <td>Chargeback</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-08 18:35:35</td>\n",
       "      <td>MexicoAccount</td>\n",
       "      <td>email205501</td>\n",
       "      <td>ip83553</td>\n",
       "      <td>card97271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   txid          bookingdate issuercountrycode txvariantcode       bin  \\\n",
       "0     1  2015-11-09 14:26:51                MX      mccredit  530056.0   \n",
       "1     2  2015-11-09 14:27:38                MX      mccredit  547046.0   \n",
       "2     3  2015-11-23 16:34:16                MX      mccredit  528843.0   \n",
       "3     4  2015-11-23 16:34:51                MX      mccredit  547146.0   \n",
       "4     5  2015-11-09 14:26:08                MX   visaclassic  477291.0   \n",
       "\n",
       "     amount currencycode shoppercountrycode shopperinteraction simple_journal  \\\n",
       "0   64800.0          MXN                 MX          Ecommerce     Chargeback   \n",
       "1   44900.0          MXN                 MX          Ecommerce     Chargeback   \n",
       "2  149900.0          MXN                 MX          Ecommerce     Chargeback   \n",
       "3  109900.0          MXN                 MX          Ecommerce     Chargeback   \n",
       "4   89900.0          MXN                 MX          Ecommerce     Chargeback   \n",
       "\n",
       "  cardverificationcodesupplied  cvcresponsecode         creationdate  \\\n",
       "0                         True                0  2015-07-01 23:03:11   \n",
       "1                         True                0  2015-07-02 04:50:55   \n",
       "2                         True                0  2015-07-02 14:30:28   \n",
       "3                         True                0  2015-07-03 07:53:37   \n",
       "4                         True                0  2015-07-08 18:35:35   \n",
       "\n",
       "     accountcode      mail_id     ip_id     card_id  \n",
       "0  MexicoAccount   email68370  ip111778  card184798  \n",
       "1  MexicoAccount  email101299   ip78749  card151595  \n",
       "2  MexicoAccount  email278604   ip70594  card242142  \n",
       "3  MexicoAccount   email47409  ip113648  card181744  \n",
       "4  MexicoAccount  email205501   ip83553   card97271  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply currency conversion so that the amounts are standardized\n",
    "def conv(row):\n",
    "    currency_dict = {0: 0.86248, 1: 1.5911, 2: 21.2829, 3: 1.6805, 4: 10.635}\n",
    "    return row['amount'] / (currency_dict[row['currencycode']]*100)\n",
    "\n",
    "def preprocess(df):\n",
    "    \n",
    "    df1 = df.copy(deep=True)\n",
    "    df1 = df1.loc[~(df1['simple_journal'] == 'Refused')] #removing the unknown class.\n",
    "    df1.loc[df1['simple_journal'] == 'Chargeback', 'simple_journal'] = 1 #fraud\n",
    "    df1.loc[df1['simple_journal'] == 'Settled', 'simple_journal'] = 0 #Normal\n",
    "    \n",
    "    #Dealing with na values by filling them with default values. \n",
    "    df1.loc[df1['cardverificationcodesupplied'].isna(),'cardverificationcodesupplied'] = False\n",
    "    df1.loc[df1['issuercountrycode'].isna(),'issuercountrycode'] = 'ZZ'\n",
    "    df1.loc[df1['shoppercountrycode'].isna(),'shoppercountrycode'] = 'ZZ'\n",
    "    df1.loc[df1['mail_id'].str.contains('na',case=False),'mail_id'] = 'email99999'\n",
    "    #3-6 is just 3 for cvcresponsecode.\n",
    "    df1.loc[df1['cvcresponsecode'] > 2,'cvcresponsecode'] = 3\n",
    "    \n",
    "    #Encoding the categorical variables.\n",
    "    bin_enc = LabelEncoder()\n",
    "    bin_enc.fit(df1['bin'].unique())\n",
    "    df1['bin'] = bin_enc.transform(df1.bin)\n",
    "\n",
    "    card_enc = LabelEncoder()\n",
    "    card_enc.fit(df1['card_id'])\n",
    "    df1['card_id'] = card_enc.transform(df1.card_id)\n",
    "\n",
    "    ip_enc = LabelEncoder()\n",
    "    ip_enc.fit(df1['ip_id'])\n",
    "    df1['ip_id'] = ip_enc.transform(df1.ip_id)\n",
    "\n",
    "    unique_issuer_cc = df1['issuercountrycode'].unique()\n",
    "    unique_shopper_cc = df1['shoppercountrycode'].unique()\n",
    "    both = np.append(unique_issuer_cc, unique_shopper_cc)\n",
    "    df_countrycodes = pd.DataFrame(both)\n",
    "    unique_codes = df_countrycodes[0].unique()\n",
    "    enc = LabelEncoder()\n",
    "    enc.fit(unique_codes)\n",
    "    df1['issuercountrycode'] = enc.transform(df1.issuercountrycode)\n",
    "    df1['shoppercountrycode'] = enc.transform(df1.shoppercountrycode)\n",
    "    \n",
    "    enc1 = LabelEncoder()\n",
    "    enc1.fit(df1['txvariantcode'])\n",
    "    df1['txvariantcode'] = enc1.transform(df1.txvariantcode)\n",
    "\n",
    "    enc2 = LabelEncoder()\n",
    "    enc2.fit(df1['currencycode'])\n",
    "    df1['currencycode'] = enc2.transform(df1.currencycode)\n",
    "\n",
    "    enc3 = LabelEncoder()\n",
    "    enc3.fit(df1['shopperinteraction'])\n",
    "    df1['shopperinteraction'] = enc3.transform(df1.shopperinteraction)\n",
    "\n",
    "    enc4 = LabelEncoder()\n",
    "    enc4.fit(df1['accountcode'])\n",
    "    df1['accountcode'] = enc4.transform(df1.accountcode)\n",
    "\n",
    "    enc5 = LabelEncoder()\n",
    "    enc5.fit(df1['cardverificationcodesupplied'])\n",
    "    df1['cardverificationcodesupplied'] = enc5.transform(df1.cardverificationcodesupplied)\n",
    "    \n",
    "    df1['creationdate'] = pd.to_datetime(df1['creationdate'])\n",
    "    df1['date'] = df1['creationdate'].dt.date\n",
    "        \n",
    "    enc6 = LabelEncoder()\n",
    "    enc6.fit(df1['mail_id'])\n",
    "    df1['mail_id'] = enc6.transform(df1.mail_id)\n",
    "\n",
    "    df1['bookingdate'] = pd.to_datetime(df1['bookingdate'])\n",
    "\n",
    "    df1['countries_equal'] = (df1['shoppercountrycode'] == df1['issuercountrycode'])\n",
    "    df1.loc[df1['countries_equal'] == False,'countries_equal'] = 0\n",
    "    df1.loc[df1['countries_equal'] == True,'countries_equal'] = 1\n",
    "\n",
    "    df1['day_of_week'] = df1['creationdate'].dt.dayofweek\n",
    "\n",
    "    df1['hour'] = df1['creationdate'].dt.hour\n",
    "    \n",
    "    dates= df1[\"creationdate\"]\n",
    "    dates=pd.DatetimeIndex(dates)\n",
    "    dates=dates.astype(np.int64) // 10**9\n",
    "    df1['creationdate_Unix']=dates\n",
    "    \n",
    "    df1['amount_eur'] = df1.apply(lambda x: conv(x), axis=1)\n",
    "    df1 = df1.loc[~((df1['simple_journal'] == 0)&(df1['amount_eur']>320))]\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote(n,k,df,y):\n",
    "\n",
    "    df1=df.copy(deep=True)\n",
    "    \n",
    "    df1[\"simple_journal\"]=y\n",
    "    \n",
    "    df_fraud= df1.loc[df1[\"simple_journal\"]==1]\n",
    "    \n",
    "    df_notfraud = df1.loc[df1[\"simple_journal\"]==0]\n",
    "    \n",
    "    df_fraudmat = df_fraud.values\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='brute').fit(df_fraudmat)\n",
    "    distances, indices = nbrs.kneighbors(df_fraudmat)\n",
    "    \n",
    "    synthetic=[]\n",
    "    \n",
    "    T = len(df_fraudmat)\n",
    "    N = n\n",
    "    \n",
    "    for i in range(T):\n",
    "        value = random.sample(range(1, k), N)\n",
    "        for j in value:\n",
    "            ind = indices[i][j]\n",
    "            diff= df_fraudmat[ind]-df_fraudmat[i]\n",
    "            synthetic.append((df_fraudmat[i]+(random.uniform(0, 1))*diff))#.astype(int))\n",
    "\n",
    "    \n",
    "    synthetic=pd.DataFrame(synthetic,columns=df_fraud.columns)\n",
    "    \n",
    "    \n",
    "    DF_syn_nf = pd.concat([synthetic, df_notfraud],sort=False)\n",
    "    \n",
    "    df_syn_nf_mat = DF_syn_nf.values\n",
    "    nbrs = NearestNeighbors(n_neighbors=2, algorithm='auto').fit(df_syn_nf_mat)\n",
    "    distances, indices = nbrs.kneighbors(df_syn_nf_mat[:len(synthetic)])\n",
    "    \n",
    "    notneeded=[]\n",
    "    for i in indices:\n",
    "        if i[1]>len(synthetic):\n",
    "            notneeded.append(i[0])\n",
    "    needed = list(set(np.arange(len(synthetic))) - set(notneeded))\n",
    "    needed_df = pd.DataFrame([synthetic.iloc[i] for i in needed])\n",
    "    \n",
    "    DF_Fraud = pd.concat([needed_df,df_fraud],sort=False)\n",
    "    \n",
    "    DF=pd.concat([DF_Fraud, df_notfraud],sort=False)\n",
    "    \n",
    "    for i in DF.columns:\n",
    "        if i!=\"amount_eur\":\n",
    "            DF[i]=DF[i].astype(int)\n",
    "    \n",
    "    return DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsampling keeping 10% fraud and 90% non fraud cases\n",
    "def subsample(k,x,y):\n",
    "    s=int(y.sum())\n",
    "    posindex=[]\n",
    "    for i in enumerate(y):\n",
    "        if i[1]==1:\n",
    "            posindex.append(i[0])\n",
    "    fullindex=list(range(0,len(y)))\n",
    "    sampleindex= list(set(fullindex)-set(posindex))\n",
    "    randomlist = list(set(random.sample(range(0, len(sampleindex)),int(s*k-s))))\n",
    "    randomlist = list(np.array(sampleindex)[randomlist])\n",
    "    randomlist.extend(posindex)\n",
    "    random.shuffle(randomlist)\n",
    "    X_train=(x.iloc[randomlist])\n",
    "    y_train=np.array(y)\n",
    "    y_train=y[randomlist].ravel()\n",
    "    return X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txid</th>\n",
       "      <th>bookingdate</th>\n",
       "      <th>issuercountrycode</th>\n",
       "      <th>txvariantcode</th>\n",
       "      <th>bin</th>\n",
       "      <th>amount</th>\n",
       "      <th>currencycode</th>\n",
       "      <th>shoppercountrycode</th>\n",
       "      <th>shopperinteraction</th>\n",
       "      <th>simple_journal</th>\n",
       "      <th>...</th>\n",
       "      <th>accountcode</th>\n",
       "      <th>mail_id</th>\n",
       "      <th>ip_id</th>\n",
       "      <th>card_id</th>\n",
       "      <th>date</th>\n",
       "      <th>countries_equal</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>creationdate_Unix</th>\n",
       "      <th>amount_eur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-11-09 14:26:51</td>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "      <td>1998</td>\n",
       "      <td>64800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>187702</td>\n",
       "      <td>7386</td>\n",
       "      <td>73358</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1435791791</td>\n",
       "      <td>30.446979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-11-09 14:27:38</td>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "      <td>2364</td>\n",
       "      <td>44900.0</td>\n",
       "      <td>2</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>834</td>\n",
       "      <td>194109</td>\n",
       "      <td>44739</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1435812655</td>\n",
       "      <td>21.096749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-11-23 16:34:16</td>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "      <td>1965</td>\n",
       "      <td>149900.0</td>\n",
       "      <td>2</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>119287</td>\n",
       "      <td>189655</td>\n",
       "      <td>122802</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1435847428</td>\n",
       "      <td>70.432131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-11-23 16:34:51</td>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "      <td>2374</td>\n",
       "      <td>109900.0</td>\n",
       "      <td>2</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>173561</td>\n",
       "      <td>8254</td>\n",
       "      <td>70712</td>\n",
       "      <td>2015-07-03</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1435910017</td>\n",
       "      <td>51.637700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-11-09 14:26:08</td>\n",
       "      <td>104</td>\n",
       "      <td>6</td>\n",
       "      <td>1301</td>\n",
       "      <td>89900.0</td>\n",
       "      <td>2</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>70647</td>\n",
       "      <td>196247</td>\n",
       "      <td>208481</td>\n",
       "      <td>2015-07-08</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1436380535</td>\n",
       "      <td>42.240484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   txid         bookingdate  issuercountrycode  txvariantcode   bin    amount  \\\n",
       "0     1 2015-11-09 14:26:51                104              2  1998   64800.0   \n",
       "1     2 2015-11-09 14:27:38                104              2  2364   44900.0   \n",
       "2     3 2015-11-23 16:34:16                104              2  1965  149900.0   \n",
       "3     4 2015-11-23 16:34:51                104              2  2374  109900.0   \n",
       "4     5 2015-11-09 14:26:08                104              6  1301   89900.0   \n",
       "\n",
       "   currencycode  shoppercountrycode  shopperinteraction  simple_journal  ...  \\\n",
       "0             2                 104                   1               1  ...   \n",
       "1             2                 104                   1               1  ...   \n",
       "2             2                 104                   1               1  ...   \n",
       "3             2                 104                   1               1  ...   \n",
       "4             2                 104                   1               1  ...   \n",
       "\n",
       "   accountcode  mail_id   ip_id  card_id        date  countries_equal  \\\n",
       "0            1   187702    7386    73358  2015-07-01                1   \n",
       "1            1      834  194109    44739  2015-07-02                1   \n",
       "2            1   119287  189655   122802  2015-07-02                1   \n",
       "3            1   173561    8254    70712  2015-07-03                1   \n",
       "4            1    70647  196247   208481  2015-07-08                1   \n",
       "\n",
       "   day_of_week hour  creationdate_Unix  amount_eur  \n",
       "0            2   23         1435791791   30.446979  \n",
       "1            3    4         1435812655   21.096749  \n",
       "2            3   14         1435847428   70.432131  \n",
       "3            4    7         1435910017   51.637700  \n",
       "4            2   18         1436380535   42.240484  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= df.drop(columns=['txid','creationdate','bookingdate','amount','date','shopperinteraction','cardverificationcodesupplied','mail_id','creationdate_Unix','card_id','countries_equal','day_of_week','currencycode'])#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop(columns=\"simple_journal\").values\n",
    "y = df2[\"simple_journal\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: (211952,) TEST: (23551,)\n",
      "TRAIN: (211952,) TEST: (23551,)\n",
      "TRAIN: (211952,) TEST: (23551,)\n",
      "TRAIN: (211953,) TEST: (23550,)\n",
      "TRAIN: (211953,) TEST: (23550,)\n",
      "TRAIN: (211953,) TEST: (23550,)\n",
      "TRAIN: (211953,) TEST: (23550,)\n",
      "TRAIN: (211953,) TEST: (23550,)\n",
      "TRAIN: (211953,) TEST: (23550,)\n",
      "TRAIN: (211953,) TEST: (23550,)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, )\n",
    "skf.get_n_splits(X, y)\n",
    "train_folds = []\n",
    "test_folds = []\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n",
    "    x_train_fold, x_test_fold = X[train_index], X[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "    train_fold = np.concatenate((x_train_fold, y_train_fold.reshape(-1, 1)), axis=1)\n",
    "    test_fold = np.concatenate((x_test_fold, y_test_fold.reshape(-1, 1)), axis=1)\n",
    "    train_folds.append(train_fold)\n",
    "    test_folds.append(test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['issuercountrycode', 'txvariantcode', 'bin', 'shoppercountrycode',\n",
       "       'cvcresponsecode', 'accountcode', 'ip_id', 'hour', 'amount_eur'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = df2.drop(columns=[\"simple_journal\"])\n",
    "col = df_temp.columns\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211952, 9)\n",
      "(23551, 9)\n",
      "310.0\n",
      "35.0\n",
      "(213057, 9)\n",
      "(23551, 9)\n",
      "1415\n",
      "35.0\n",
      "(7075, 9)\n",
      "(23551, 9)\n",
      "1415\n",
      "35.0\n",
      "(211952, 9)\n",
      "(23551, 9)\n",
      "310.0\n",
      "35.0\n",
      "(212870, 9)\n",
      "(23551, 9)\n",
      "1228\n",
      "35.0\n",
      "(6140, 9)\n",
      "(23551, 9)\n",
      "1228\n",
      "35.0\n",
      "(211952, 9)\n",
      "(23551, 9)\n",
      "310.0\n",
      "35.0\n",
      "(212845, 9)\n",
      "(23551, 9)\n",
      "1203\n",
      "35.0\n",
      "(6015, 9)\n",
      "(23551, 9)\n",
      "1203\n",
      "35.0\n",
      "(211953, 9)\n",
      "(23550, 9)\n",
      "310.0\n",
      "35.0\n",
      "(212850, 9)\n",
      "(23550, 9)\n",
      "1207\n",
      "35.0\n",
      "(6035, 9)\n",
      "(23550, 9)\n",
      "1207\n",
      "35.0\n",
      "(211953, 9)\n",
      "(23550, 9)\n",
      "310.0\n",
      "35.0\n",
      "(212873, 9)\n",
      "(23550, 9)\n",
      "1230\n",
      "35.0\n",
      "(6150, 9)\n",
      "(23550, 9)\n",
      "1230\n",
      "35.0\n",
      "(211953, 9)\n",
      "(23550, 9)\n",
      "311.0\n",
      "34.0\n",
      "(212859, 9)\n",
      "(23550, 9)\n",
      "1217\n",
      "34.0\n",
      "(6085, 9)\n",
      "(23550, 9)\n",
      "1217\n",
      "34.0\n",
      "(211953, 9)\n",
      "(23550, 9)\n",
      "311.0\n",
      "34.0\n",
      "(212854, 9)\n",
      "(23550, 9)\n",
      "1212\n",
      "34.0\n",
      "(6060, 9)\n",
      "(23550, 9)\n",
      "1212\n",
      "34.0\n",
      "(211953, 9)\n",
      "(23550, 9)\n",
      "311.0\n",
      "34.0\n",
      "(212851, 9)\n",
      "(23550, 9)\n",
      "1209\n",
      "34.0\n",
      "(6045, 9)\n",
      "(23550, 9)\n",
      "1209\n",
      "34.0\n",
      "(211953, 9)\n",
      "(23550, 9)\n",
      "311.0\n",
      "34.0\n",
      "(212893, 9)\n",
      "(23550, 9)\n",
      "1251\n",
      "34.0\n",
      "(6255, 9)\n",
      "(23550, 9)\n",
      "1251\n",
      "34.0\n",
      "(211953, 9)\n",
      "(23550, 9)\n",
      "311.0\n",
      "34.0\n",
      "(212851, 9)\n",
      "(23550, 9)\n",
      "1209\n",
      "34.0\n",
      "(6045, 9)\n",
      "(23550, 9)\n",
      "1209\n",
      "34.0\n"
     ]
    }
   ],
   "source": [
    "for k,l in zip(train_folds,test_folds):\n",
    "    train_X = k[:,:-1]\n",
    "    train_y = np.ravel(k[:,-1:])\n",
    "    test_X = l[:,:-1]\n",
    "    test_y = np.ravel(l[:,-1:])\n",
    "    print(train_X.shape)\n",
    "    print(test_X.shape)\n",
    "    print(train_y.sum())\n",
    "    print(test_y.sum())\n",
    "    train_X=pd.DataFrame(train_X,columns=col)\n",
    "    df_smote = smote(5,6,train_X,train_y)\n",
    "    train_X = df_smote.drop(columns=\"simple_journal\")\n",
    "    train_y = df_smote[\"simple_journal\"]\n",
    "    print(train_X.shape)\n",
    "    print(test_X.shape)\n",
    "    print(train_y.sum())\n",
    "    print(test_y.sum())\n",
    "    train_X,train_y = subsample(5,train_X,np.array(train_y))\n",
    "    print(train_X.shape)\n",
    "    print(test_X.shape)\n",
    "    print(train_y.sum())\n",
    "    print(test_y.sum())\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler =StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import decomposition\n",
    "#pca = decomposition.PCA(n_components=3)\n",
    "#pca.fit(X_train)\n",
    "#X_train =pca.transform(X_train)\n",
    "#pca.fit(X_test)\n",
    "#X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.sum(),X_test.shape,y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty=\"elasticnet\",class_weight=\"balanced\",max_iter=1000,solver=\"saga\",l1_ratio=0.9,C=.1)\n",
    "\n",
    "\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" %(X_test.shape[0], (y_test != y_pred).sum()))\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=\"AUC for not rank-swapped data= \" +str(round(auc*100,2)))\n",
    "\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n",
    "(tn, fp, fn, tp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty=\"elasticnet\",max_iter=100,solver=\"saga\",l1_ratio=0.9,C=.1)\n",
    "\n",
    "\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" %(X_test.shape[0], (y_test != y_pred).sum()))\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=\"AUC for not rank-swapped data= \" +str(round(auc*100,2)))\n",
    "\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import Tensor\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "dataset = TensorDataset( Tensor(X_train), Tensor(y_train).type(torch.LongTensor) )\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "batch_size_train = 200\n",
    "\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight=[]\n",
    "for i in y_train:\n",
    "    if i==0:\n",
    "        samples_weight.append(weight[0])\n",
    "    else:\n",
    "        samples_weight.append(weight[1])\n",
    "samples_weight=np.array(samples_weight)\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "samples_weigth = samples_weight.double()\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size_train, shuffle=False,num_workers=4,sampler=sampler)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(32, 64)\n",
    "        self.fc4 = nn.Linear(64,32)\n",
    "        self.fc5 = nn.Linear(32,16)\n",
    "        self.fc6 = nn.Linear(16,8)\n",
    "        self.fc7 = nn.Linear(8,4)\n",
    "        self.fc8 = nn.Linear(4,2)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc1.weight)  # glorot\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)  # glorot\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)  # glorot\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)  # glorot\n",
    "        nn.init.zeros_(self.fc4.bias)\n",
    "        nn.init.xavier_uniform_(self.fc5.weight)  # glorot\n",
    "        nn.init.zeros_(self.fc5.bias)\n",
    "        nn.init.xavier_uniform_(self.fc6.weight)  # glorot\n",
    "        nn.init.zeros_(self.fc6.bias)\n",
    "        nn.init.xavier_uniform_(self.fc7.weight)  # glorot\n",
    "        nn.init.zeros_(self.fc7.bias)\n",
    "        nn.init.xavier_uniform_(self.fc8.weight)  # glorot\n",
    "        nn.init.zeros_(self.fc8.bias)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "if use_gpu:\n",
    "    net = net.cuda()\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.5)\n",
    "\n",
    "net.train()\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))                                                            \n",
    "            \n",
    "            running_loss = 0.0\n",
    "            niter = epoch * len(trainloader) + i\n",
    "\n",
    "print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = TensorDataset( Tensor(X_test), Tensor(np.array(y_test)).type(torch.LongTensor) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLoader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=len(X_test),\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: ' + str(\n",
    "    100 * correct / total) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=predicted.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test,predicted).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
